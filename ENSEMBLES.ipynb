{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403A51L33/B52_PfDS/blob/main/ENSEMBLES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHWyio8ieYWS",
        "outputId": "8efe62c8-5db9-4fdc-a045-59aa428fbed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training base model: logreg\n",
            "  logreg macro-F1: 0.3344953782305226\n",
            "Training base model: svm\n",
            "  svm macro-F1: 0.3025114098926853\n",
            "Training base model: rf\n",
            "  rf macro-F1: 0.32921622135331813\n",
            "Training base model: et\n",
            "  et macro-F1: 0.36134290546055253\n",
            "Training base model: cnb\n",
            "  cnb macro-F1: 0.35690058479532166\n",
            "Tuning Voting weights...\n",
            "Best voting weights: {'weights': [2, 2, 1, 1, 1]}\n",
            "Voting macro-F1: 0.3628403394850763\n",
            "Training Stacking ensemble...\n",
            "Stacking macro-F1: 0.33994565217391304\n",
            "Computing permutation importance for Stacking...\n",
            "\n",
            "Done. Artifacts saved in: ./ensemble_outputs\n",
            "Top files to check:\n",
            " - metrics_.json, confusion_matrix_.json (each base model)\n",
            " - metrics_voting.json, metrics_stacking.json\n",
            " - permutation_importance_stacking.csv\n",
            " - feature_importances_rf.csv / feature_importances_et.csv (if available)\n",
            " - feature_names.txt\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Ensembles for Explainable Drug Risk Classification\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
        "\n",
        "# --------------------------\n",
        "# Reproducibility & paths\n",
        "# --------------------------\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "DATA_PATH = \"/mnt/data/realistic_drug_labels_side_effects.csv\"\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    alt = \"./realistic_drug_labels_side_effects.csv\"\n",
        "    if os.path.exists(alt):\n",
        "        DATA_PATH = alt\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"CSV not found at {DATA_PATH} or {alt}\")\n",
        "\n",
        "OUTDIR = \"./ensemble_outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# --------------------------\n",
        "# Load data\n",
        "# --------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Guess columns (adjust here if needed)\n",
        "TEXT_COLS = [c for c in [\"indications\", \"side_effects\", \"contraindications\", \"warnings\"] if c in df.columns]\n",
        "NUM_COLS  = [c for c in [\"dosage_mg\", \"price_usd\", \"approval_year\"] if c in df.columns]\n",
        "CAT_COLS  = [c for c in [\"drug_class\", \"administration_route\", \"approval_status\", \"manufacturer\"] if c in df.columns]\n",
        "\n",
        "# Fallback if no numeric/categorical columns exist\n",
        "if len(TEXT_COLS) == 0:\n",
        "    # Create a synthetic text field from all string-like columns\n",
        "    TEXT_COLS = [c for c in df.columns if df[c].dtype == object and c != \"side_effect_severity\"]\n",
        "\n",
        "for c in TEXT_COLS: df[c] = df[c].fillna(\"\")\n",
        "for c in NUM_COLS:  df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "for c in CAT_COLS:  df[c] = df[c].astype(str).fillna(\"UNK\")\n",
        "\n",
        "# --------------------------\n",
        "# Target engineering\n",
        "# --------------------------\n",
        "def _to_num(v):\n",
        "    try:\n",
        "        return float(v)\n",
        "    except:\n",
        "        m = {\"low\":0, \"mild\":0, \"moderate\":1, \"medium\":1, \"high\":2, \"severe\":2}\n",
        "        return m.get(str(v).strip().lower(), np.nan)\n",
        "\n",
        "y_raw = df[\"side_effect_severity\"]\n",
        "if y_raw.dtype.kind in \"ifu\":\n",
        "    q = np.quantile(y_raw, [0.33, 0.66])\n",
        "    y = y_raw.apply(lambda v: 0 if v <= q[0] else (1 if v <= q[1] else 2)).astype(int).values\n",
        "else:\n",
        "    tmp = y_raw.apply(_to_num)\n",
        "    if tmp.isna().mean() < 0.5:\n",
        "        q = np.quantile(tmp.fillna(tmp.median()), [0.33, 0.66])\n",
        "        y = tmp.fillna(tmp.median()).apply(lambda v: 0 if v <= q[0] else (1 if v <= q[1] else 2)).astype(int).values\n",
        "    else:\n",
        "        cats = {k:i for i,k in enumerate(sorted(y_raw.astype(str).unique()))}\n",
        "        y = y_raw.astype(str).map(cats).values % 3\n",
        "\n",
        "CLASS_NAMES = [\"low\",\"moderate\",\"high\"]\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "# --------------------------\n",
        "# Feature union via ColumnTransformer\n",
        "# --------------------------\n",
        "def concat_text_columns(X):\n",
        "    # X is a DataFrame\n",
        "    return X[TEXT_COLS].apply(lambda r: \" \".join(map(str, r.values)), axis=1)\n",
        "\n",
        "text_union = Pipeline([\n",
        "    (\"text_concat\", FunctionTransformer(concat_text_columns, validate=False)),\n",
        "    (\"tfidf\", TfidfVectorizer(max_features=50000, ngram_range=(1,2), min_df=2))\n",
        "])\n",
        "\n",
        "num_proc = Pipeline([\n",
        "    (\"impute\", FunctionTransformer(lambda x: np.nan_to_num(x, nan=np.nanmedian(x, axis=0)), accept_sparse=True)),\n",
        "    (\"scale\", StandardScaler(with_mean=False))  # with_mean=False keeps sparse compatibility if any\n",
        "])\n",
        "\n",
        "cat_proc = Pipeline([\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=5))\n",
        "])\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"text\", text_union, TEXT_COLS),\n",
        "        (\"num\", num_proc, NUM_COLS) if len(NUM_COLS)>0 else (\"num\", \"drop\", []),\n",
        "        (\"cat\", cat_proc, CAT_COLS) if len(CAT_COLS)>0 else (\"cat\", \"drop\", []),\n",
        "    ],\n",
        "    sparse_threshold=0.3\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Train/test split\n",
        "# --------------------------\n",
        "X = df[TEXT_COLS + NUM_COLS + CAT_COLS]\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Class weights for imbalance\n",
        "classes = np.unique(y_train)\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
        "CLASS_WEIGHTS = {c:w for c,w in zip(classes, cw)}\n",
        "\n",
        "# --------------------------\n",
        "# Base models (wrapped in pipelines)\n",
        "# --------------------------\n",
        "# 1) Logistic Regression (saga handles large sparse, L2)\n",
        "logreg = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        class_weight=\"balanced\", solver=\"saga\", max_iter=2000, C=2.0, n_jobs=1, random_state=SEED))\n",
        "])\n",
        "\n",
        "# 2) Linear SVM + calibration for probabilities\n",
        "svm_base = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"svm\", LinearSVC(class_weight=\"balanced\", C=1.0, random_state=SEED))\n",
        "])\n",
        "svm = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"cal\", CalibratedClassifierCV(estimator=LinearSVC(\n",
        "        class_weight=\"balanced\", C=1.0, random_state=SEED\n",
        "    ), method=\"sigmoid\", cv=3))\n",
        "])\n",
        "\n",
        "# 3) Random Forest\n",
        "rf = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", RandomForestClassifier(\n",
        "        n_estimators=400, max_depth=None, min_samples_split=2,\n",
        "        class_weight=\"balanced_subsample\", n_jobs=-1, random_state=SEED))\n",
        "])\n",
        "\n",
        "# 4) Extra Trees\n",
        "et = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", ExtraTreesClassifier(\n",
        "        n_estimators=600, max_depth=None, min_samples_split=2,\n",
        "        class_weight=\"balanced\", n_jobs=-1, random_state=SEED))\n",
        "])\n",
        "\n",
        "# 5) Complement Naive Bayes (good for text)\n",
        "# Note: Requires non-negative features (TF-IDF + one-hot are non-negative; StandardScaler with_mean=False keeps >=0)\n",
        "cnb = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", ComplementNB(alpha=0.3))\n",
        "])\n",
        "\n",
        "BASE_MODELS = {\n",
        "    \"logreg\": logreg,\n",
        "    \"svm\": svm,\n",
        "    \"rf\": rf,\n",
        "    \"et\": et,\n",
        "    \"cnb\": cnb\n",
        "}\n",
        "\n",
        "# --------------------------\n",
        "# Fit base models\n",
        "# --------------------------\n",
        "for name, pipe in BASE_MODELS.items():\n",
        "    print(f\"Training base model: {name}\")\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    rep = classification_report(y_test, y_pred, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=[0,1,2]).tolist()\n",
        "    with open(os.path.join(OUTDIR, f\"metrics_{name}.json\"), \"w\") as f:\n",
        "        json.dump(rep, f, indent=2)\n",
        "    with open(os.path.join(OUTDIR, f\"confusion_matrix_{name}.json\"), \"w\") as f:\n",
        "        json.dump(cm, f, indent=2)\n",
        "    print(f\"  {name} macro-F1:\", rep[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "# --------------------------\n",
        "# SOFT VOTING ENSEMBLE\n",
        "# --------------------------\n",
        "voters = []\n",
        "for name in [\"logreg\",\"svm\",\"rf\",\"et\",\"cnb\"]:\n",
        "    if name in BASE_MODELS:\n",
        "        voters.append((name, BASE_MODELS[name].steps[-1][1] if isinstance(BASE_MODELS[name], Pipeline) else BASE_MODELS[name]))\n",
        "\n",
        "# IMPORTANT: We must reuse the SAME preprocessing for voting/stacking.\n",
        "# We'll create a shared preprocessor and clone base estimators without their pre step for Voting.\n",
        "# To keep it simple and robust, wrap them AGAIN in a full pipeline but use Voting on calibrated models.\n",
        "voting = VotingClassifier(\n",
        "    estimators=[\n",
        "        (\"logreg\", logreg),\n",
        "        (\"svm\", svm),\n",
        "        (\"rf\", rf),\n",
        "        (\"et\", et),\n",
        "        (\"cnb\", cnb)\n",
        "    ],\n",
        "    voting=\"soft\",\n",
        "    weights=[2,2,2,2,1],  # initial weights; tuned briefly below\n",
        "    n_jobs=-1,\n",
        "    flatten_transform=True\n",
        ")\n",
        "\n",
        "# Small randomized search over weights (integers 1..3) to improve soft voting\n",
        "param_dist = {\n",
        "    \"weights\": [\n",
        "        [a,b,c,d,e] for a in [1,2,3]\n",
        "                    for b in [1,2,3]\n",
        "                    for c in [1,2,3]\n",
        "                    for d in [1,2,3]\n",
        "                    for e in [1,2,3]\n",
        "    ]\n",
        "}\n",
        "# Limit samples to keep it quick\n",
        "sampled = random.sample(param_dist[\"weights\"], k=min(25, len(param_dist[\"weights\"])))\n",
        "search = RandomizedSearchCV(\n",
        "    voting, param_distributions={\"weights\": [w for w in sampled]},\n",
        "    n_iter=len(sampled), scoring=\"f1_macro\", cv=3, n_jobs=-1, random_state=SEED, verbose=0\n",
        ")\n",
        "print(\"Tuning Voting weights...\")\n",
        "search.fit(X_train, y_train)\n",
        "voting_best = search.best_estimator_\n",
        "print(\"Best voting weights:\", search.best_params_)\n",
        "\n",
        "# Evaluate Voting\n",
        "y_pred_v = voting_best.predict(X_test)\n",
        "rep_v = classification_report(y_test, y_pred_v, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
        "cm_v = confusion_matrix(y_test, y_pred_v, labels=[0,1,2]).tolist()\n",
        "with open(os.path.join(OUTDIR, \"metrics_voting.json\"), \"w\") as f:\n",
        "    json.dump(rep_v, f, indent=2)\n",
        "with open(os.path.join(OUTDIR, \"confusion_matrix_voting.json\"), \"w\") as f:\n",
        "    json.dump(cm_v, f, indent=2)\n",
        "print(\"Voting macro-F1:\", rep_v[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "# --------------------------\n",
        "# STACKING ENSEMBLE\n",
        "# --------------------------\n",
        "stack = StackingClassifier(\n",
        "    estimators=[\n",
        "        (\"logreg\", logreg),\n",
        "        (\"svm\", svm),\n",
        "        (\"rf\", rf),\n",
        "        (\"et\", et),\n",
        "        (\"cnb\", cnb)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(solver=\"lbfgs\", max_iter=2000, class_weight=\"balanced\"),\n",
        "    stack_method=\"predict_proba\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "\n",
        "print(\"Training Stacking ensemble...\")\n",
        "stack.fit(X_train, y_train)\n",
        "y_pred_s = stack.predict(X_test)\n",
        "rep_s = classification_report(y_test, y_pred_s, target_names=CLASS_NAMES, output_dict=True, zero_division=0)\n",
        "cm_s = confusion_matrix(y_test, y_pred_s, labels=[0,1,2]).tolist()\n",
        "with open(os.path.join(OUTDIR, \"metrics_stacking.json\"), \"w\") as f:\n",
        "    json.dump(rep_s, f, indent=2)\n",
        "with open(os.path.join(OUTDIR, \"confusion_matrix_stacking.json\"), \"w\") as f:\n",
        "    json.dump(cm_s, f, indent=2)\n",
        "print(\"Stacking macro-F1:\", rep_s[\"macro avg\"][\"f1-score\"])\n",
        "\n",
        "# --------------------------\n",
        "# Explainability\n",
        "# --------------------------\n",
        "# Helper: extract feature names from the ColumnTransformer\n",
        "def get_feature_names(preprocessor: ColumnTransformer):\n",
        "    feature_names = []\n",
        "    for name, transformer, cols in preprocessor.transformers_:\n",
        "        if name == \"remainder\":\n",
        "            continue\n",
        "        if transformer == \"drop\":\n",
        "            continue\n",
        "        if hasattr(transformer, \"named_steps\"):\n",
        "            last = list(transformer.named_steps.values())[-1]\n",
        "            # TF-IDF branch\n",
        "            if isinstance(last, TfidfVectorizer):\n",
        "                feature_names += [f\"text__{t}\" for t in last.get_feature_names_out()]\n",
        "            # OneHot branch\n",
        "            elif isinstance(last, OneHotEncoder):\n",
        "                try:\n",
        "                    ohe = last\n",
        "                except Exception:\n",
        "                    # if nested\n",
        "                    for step in transformer.named_steps.values():\n",
        "                        if isinstance(step, OneHotEncoder):\n",
        "                            ohe = step; break\n",
        "                if isinstance(cols, list):\n",
        "                    col_names = cols\n",
        "                else:\n",
        "                    col_names = [c for c in cols]\n",
        "                ohe_names = list(ohe.get_feature_names_out(col_names))\n",
        "                feature_names += [f\"cat__{t}\" for t in ohe_names]\n",
        "            else:\n",
        "                # Numeric scaler or other\n",
        "                if isinstance(cols, list) and len(cols)>0:\n",
        "                    feature_names += [f\"num__{c}\" for c in cols]\n",
        "        else:\n",
        "            # If transformer is directly Tfidf/OneHot/etc.\n",
        "            if isinstance(transformer, TfidfVectorizer):\n",
        "                feature_names += [f\"text__{t}\" for t in transformer.get_feature_names_out()]\n",
        "            elif isinstance(transformer, OneHotEncoder):\n",
        "                ohe_names = list(transformer.get_feature_names_out(cols))\n",
        "                feature_names += [f\"cat__{t}\" for t in ohe_names]\n",
        "            else:\n",
        "                if isinstance(cols, list) and len(cols)>0:\n",
        "                    feature_names += [f\"num__{c}\" for c in cols]\n",
        "    return np.array(feature_names)\n",
        "\n",
        "# We will explain the Stacking model using permutation importance\n",
        "# (on the full pipeline, so we need a reference preprocessor)\n",
        "reference_pre = pre  # used inside our pipelines\n",
        "\n",
        "# Save feature names for reference\n",
        "feat_names = None\n",
        "try:\n",
        "    # Fit a clone of the preprocessor alone to the training data to obtain consistent vocab/encodings\n",
        "    # (We can re-use from any fitted pipeline; choose logreg)\n",
        "    pre_fitted = BASE_MODELS[\"logreg\"].named_steps[\"pre\"]\n",
        "    feat_names = get_feature_names(pre_fitted)\n",
        "    with open(os.path.join(OUTDIR, \"feature_names.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        for n in feat_names:\n",
        "            f.write(n + \"\\n\")\n",
        "except Exception as e:\n",
        "    print(\"Feature name extraction failed:\", e)\n",
        "\n",
        "# Permutation importance on Stacking (global)\n",
        "try:\n",
        "    print(\"Computing permutation importance for Stacking...\")\n",
        "    pi = permutation_importance(stack, X_test, y_test, n_repeats=10, random_state=SEED, scoring=\"f1_macro\", n_jobs=-1)\n",
        "    imp = pd.DataFrame({\"feature\": np.arange(len(pi.importances_mean)),\n",
        "                        \"importance_mean\": pi.importances_mean,\n",
        "                        \"importance_std\":  pi.importances_std})\n",
        "    if feat_names is not None and len(feat_names) == imp.shape[0]:\n",
        "        imp[\"feature\"] = feat_names\n",
        "    imp.sort_values(\"importance_mean\", ascending=False).to_csv(os.path.join(OUTDIR, \"permutation_importance_stacking.csv\"), index=False)\n",
        "except Exception as e:\n",
        "    print(\"Permutation importance failed:\", e)\n",
        "\n",
        "# Tree-based feature importance (RF / ET) – approximate and global\n",
        "def dump_tree_importances(name, pipe):\n",
        "    try:\n",
        "        clf = pipe.named_steps[\"clf\"]\n",
        "        if hasattr(clf, \"feature_importances_\"):\n",
        "            # Need fitted preprocessor to align names\n",
        "            pre_fit = pipe.named_steps[\"pre\"]\n",
        "            names = get_feature_names(pre_fit)\n",
        "            imp = pd.DataFrame({\"feature\": names, \"importance\": clf.feature_importances_})\n",
        "            imp.sort_values(\"importance\", ascending=False).to_csv(os.path.join(OUTDIR, f\"feature_importances_{name}.csv\"), index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"{name} importances failed:\", e)\n",
        "\n",
        "dump_tree_importances(\"rf\", rf)\n",
        "dump_tree_importances(\"et\", et)\n",
        "\n",
        "# --------------------------\n",
        "# Save quick summary & OOF-like probabilities on test\n",
        "# --------------------------\n",
        "def save_summary(tag, report, cm):\n",
        "    summary = {\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
        "        \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
        "        \"per_class\": {k: report[k] for k in [\"low\",\"moderate\",\"high\"] if k in report},\n",
        "        \"confusion_matrix\": cm\n",
        "    }\n",
        "    with open(os.path.join(OUTDIR, f\"summary_{tag}.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "save_summary(\"voting\", rep_v, cm_v)\n",
        "save_summary(\"stacking\", rep_s, cm_s)\n",
        "\n",
        "# Save predicted probabilities for post-hoc threshold tuning / calibration plots\n",
        "def get_probs(model, X):\n",
        "    try:\n",
        "        return model.predict_proba(X)\n",
        "    except Exception:\n",
        "        # Some models may expose transform output when flatten_transform=True in Voting\n",
        "        pred = model.predict(X)\n",
        "        P = np.zeros((len(pred), NUM_CLASSES))\n",
        "        P[np.arange(len(pred)), pred] = 1.0\n",
        "        return P\n",
        "\n",
        "np.save(os.path.join(OUTDIR, \"oof_probs_voting.npy\"), get_probs(voting_best, X_test))\n",
        "np.save(os.path.join(OUTDIR, \"oof_probs_stacking.npy\"), get_probs(stack, X_test))\n",
        "\n",
        "print(\"\\nDone. Artifacts saved in:\", OUTDIR)\n",
        "print(\"Top files to check:\")\n",
        "print(\" - metrics_.json, confusion_matrix_.json (each base model)\")\n",
        "print(\" - metrics_voting.json, metrics_stacking.json\")\n",
        "print(\" - permutation_importance_stacking.csv\")\n",
        "print(\" - feature_importances_rf.csv / feature_importances_et.csv (if available)\")\n",
        "print(\" - feature_names.txt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnbLmqbgOCLvKAtR1Q8Og8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}